{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8fb1c6",
   "metadata": {},
   "source": [
    "### Hop-Typed Communication Model (NVLink vs NDR) for **LLaMA-3 8B**\n",
    "\n",
    "This notebook estimates the communication **latency** and **energy** incurred when training or serving LLaMA-3 8B on a DGX-H100.  Each network hop is classified as **NVLink 4** or **InfiniBand NDR 400**, allowing the model to account for the very different bandwidths, startup latencies, and energy costs of those two links.\n",
    "\n",
    "---\n",
    "\n",
    "#### Parallelism Strategies Examined\n",
    "| Strategy | Communication Pattern (per transformer block) |\n",
    "|----------|----------------------------------------------|\n",
    "| **Data Parallel (DP)** | All-reduce of full-precision gradients across all GPUs. |\n",
    "| **Tensor Parallel (TP)** | Reduce-scatter / all-reduce / all-gather on activation slices that have been sharded over GPUs. |\n",
    "| **Pipeline Parallel (PP)** | Point-to-point transfer of activation checkpoints between consecutive pipeline stages. *Pipeline bubbles are **not** modeled.* |\n",
    "\n",
    "---\n",
    "\n",
    "#### Model Scope\n",
    "- **Architecture:** LLaMA-3 8B  \n",
    "  – 32 transformer layers, hidden size = 4096, multi-query attention  \n",
    "- **Numerics:** FP16 / BF16 (2 B per element)  \n",
    "- **Sequence Length:** 2048 tokens (for activation sizing)\n",
    "\n",
    "---\n",
    "\n",
    "#### Hardware Link Constants\n",
    "| Link            | Peak BW | Startup \\( \\alpha \\) | Inverse BW \\( \\beta = 1/\\text{BW} \\) | Energy/bit |\n",
    "|-----------------|---------|----------------------|--------------------------------------|------------|\n",
    "| **NVLink 4**    | 900 GB/s | 0.2 µs | 11 ps/B | 5 pJ |\n",
    "| **NDR 400**     | 50 GB/s  | 2 µs  | 20 ps/B | 25 pJ |\n",
    "\n",
    "A message of size \\(M\\) bytes traversing \\(n_{\\text{NV}}\\) NVLink hops and \\(n_{\\text{IB}}\\) InfiniBand hops incurs  \n",
    "\n",
    "$\n",
    "\\text{Latency} = n_{\\text{NV}}\\bigl(\\alpha_{NV} + \\beta_{NV}M \\bigr) \\;+\\; n_{\\text{IB}}\\bigl(\\alpha_{IB} + \\beta_{IB}M \\bigr),\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Energy}  = 8M\\bigl(5\\,n_{\\text{NV}} + 25\\,n_{\\text{IB}}\\bigr)\\text{ pJ}.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Objectives of the Notebook\n",
    "1. **Quantify Communication Cost** — Report latency (µs) and energy (nJ) per message for each strategy and hop mix.  \n",
    "2. **Compare Parallel Schemes** — Highlight how DP, TP, and PP trade off communication time and energy under the same hardware assumptions.  \n",
    "3. **Guide Design Decisions** — Provide first-order numbers that help decide which parallelism (or combination) is appropriate for a given training or inference workload.\n",
    "\n",
    "---\n",
    "\n",
    "#### Notebook Outputs\n",
    "- **Per-Hop Latency** and **Energy** tables for each collective or point-to-point operation.  \n",
    "- **Data Volume** moved (MB) per GPU and in aggregate.  \n",
    "- Consolidated **comparative charts** for DP, TP, and PP to illustrate trade-offs.\n",
    "\n",
    "> **Caveat:**  Results are first-order; they do not model overlap, asynchronous progress, or pipeline fill/drain bubbles. They nevertheless capture the dominant communication costs needed for quick design-space exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8a9c5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "# ---------- Model sizes ----------\n",
    "BYTES_FP16 = 2\n",
    "d_model = 4096\n",
    "d_ff    = 14336\n",
    "seq_len = 2048\n",
    "\n",
    "# Self‑attention params (MQA)\n",
    "attn_elems = d_model*d_model + 2*1024*d_model + d_model*d_model\n",
    "attn_bytes = attn_elems * BYTES_FP16\n",
    "\n",
    "# MLP params (no gate)\n",
    "mlp_elems  = d_ff*d_model + d_model*d_ff\n",
    "mlp_bytes  = mlp_elems  * BYTES_FP16\n",
    "\n",
    "# Activation size (one block output) for PP / TP\n",
    "act_bytes = seq_len * d_model * BYTES_FP16\n",
    "\n",
    "layer_bytes = dict(ATTN=attn_bytes, MLP=mlp_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "de7419cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hop tuples (nv, ib)\n",
    "def hops_8():\n",
    "    H = np.empty((8,8),dtype=object)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if i==j: H[i,j]=(0,0)\n",
    "            else:    H[i,j]=(1,0)   # single NV hop for any pair\n",
    "    return H\n",
    "hop8 = hops_8()\n",
    "\n",
    "def hops_16():\n",
    "    H = np.empty((16,16),dtype=object)\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            if i==j: H[i,j]=(0,0)\n",
    "            else:\n",
    "                same_node=(i//8)==(j//8)\n",
    "                if same_node: H[i,j]=(1,0)\n",
    "                else:         H[i,j]=(2,1)  # GPU→NV + IB + NV\n",
    "    return H\n",
    "hop16 = hops_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4401d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_nv, beta_nv, epb_nv = 0.2e-6, 1/90e9, 5e-12\n",
    "alpha_ib, beta_ib, epb_ib = 2e-6, 1/50e9, 25e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09dff8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_bytes(lat_bytes, nv_hops, ib_hops):\n",
    "    lat = nv_hops*(alpha_nv+beta_nv*lat_bytes) + ib_hops*(alpha_ib+beta_ib*lat_bytes)\n",
    "    eng = nv_hops*lat_bytes*8*epb_nv + ib_hops*lat_bytes*8*epb_ib\n",
    "    return lat, eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "19087651",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "def ring_dp(layer_name, layer_b, hopmat, mode_label):\n",
    "    n = hopmat.shape[0]\n",
    "    chunk = layer_b / n                      # bytes sent per link per step\n",
    "    per_gpu_bytes = layer_b * (n - 1) / n    # textbook formula\n",
    "    steps = 2 * (n - 1)                      # reduce-scatter + all-gather\n",
    "\n",
    "    total_lat = 0.0\n",
    "    total_eng = 0.0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        max_link_lat = 0.0\n",
    "        step_energy  = 0.0\n",
    "\n",
    "        # each rank communicates with its neighbour every step\n",
    "        for r in range(n):\n",
    "            sender    = r\n",
    "            receiver  = (r + 1) % n          # fixed clockwise ring\n",
    "            nv, ib     = hopmat[sender, receiver]\n",
    "            lat, eng   = cost_bytes(chunk, nv, ib)\n",
    "\n",
    "            max_link_lat = max(max_link_lat, lat)\n",
    "            step_energy += eng               # all links consume energy\n",
    "\n",
    "        total_lat += max_link_lat            # critical-path latency\n",
    "        total_eng += step_energy\n",
    "\n",
    "    rows.append([layer_name, mode_label,\n",
    "                 per_gpu_bytes / 1e6,        # MiB\n",
    "                 total_lat  * 1e3,           # µs\n",
    "                 total_eng  * 1e3])          # nJ\n",
    "\n",
    "\n",
    "for L,B in layer_bytes.items():\n",
    "    ring_dp(L,B,hop8,\"DP‑8\")\n",
    "    ring_dp(L,B,hop16,\"DP‑16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2deb6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_cost(layer_name, bytes_per_msg):\n",
    "    # reduce‑scatter + all‑reduce = 2 msgs; each msg one NV hop\n",
    "    nv_hops,ib_hops = 1,0\n",
    "    lat1,eng1 = cost_bytes(bytes_per_msg, nv_hops, ib_hops)\n",
    "    lat=2*lat1; eng=2*eng1\n",
    "    rows.append([layer_name,\"TP‑8\",bytes_per_msg*2/1e6,lat*1e3,eng*1e3])\n",
    "\n",
    "tp_bytes = act_bytes  # 4096*seq*2\n",
    "tp_cost(\"ATTN\", tp_bytes)\n",
    "tp_cost(\"MLP\" , tp_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "009181ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_cost(label, nv_hops, ib_hops):\n",
    "    lat,eng = cost_bytes(act_bytes,nv_hops,ib_hops)\n",
    "    rows.append([\"BLOCK\",label,act_bytes/1e6,lat*1e3,eng*1e3])\n",
    "\n",
    "pp_cost(\"PP‑NV\",1,0)\n",
    "pp_cost(\"PP‑Cross\",2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "666ddda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer</th>\n",
       "      <th>Mode</th>\n",
       "      <th>PerGPU_MB</th>\n",
       "      <th>PerGPU_MiB</th>\n",
       "      <th>Latency_ms</th>\n",
       "      <th>Energy_mJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>73.400</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.634</td>\n",
       "      <td>46.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>78.643</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6.713</td>\n",
       "      <td>176.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>205.521</td>\n",
       "      <td>196.0</td>\n",
       "      <td>4.570</td>\n",
       "      <td>131.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>220.201</td>\n",
       "      <td>210.0</td>\n",
       "      <td>18.667</td>\n",
       "      <td>493.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>33.554</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.373</td>\n",
       "      <td>1.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>33.554</td>\n",
       "      <td>32.0</td>\n",
       "      <td>0.373</td>\n",
       "      <td>1.342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑NV</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑Cross</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.711</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Layer      Mode  PerGPU_MB  PerGPU_MiB  Latency_ms  Energy_mJ\n",
       "0   ATTN      DP‑8     73.400        70.0       1.634     46.976\n",
       "1   ATTN     DP‑16     78.643        75.0       6.713    176.161\n",
       "2    MLP      DP‑8    205.521       196.0       4.570    131.533\n",
       "3    MLP     DP‑16    220.201       210.0      18.667    493.250\n",
       "4   ATTN      TP‑8     33.554        32.0       0.373      1.342\n",
       "5    MLP      TP‑8     33.554        32.0       0.373      1.342\n",
       "6  BLOCK     PP‑NV     16.777        16.0       0.187      0.671\n",
       "7  BLOCK  PP‑Cross     16.777        16.0       0.711      4.698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.DataFrame(rows,columns=[\"Layer\",\"Mode\",\"PerGPU_MB\",\"Latency_ms\",\"Energy_mJ\"])\n",
    "df.round(3)\n",
    "\n",
    "# Derive group size from the Mode label\n",
    "def _group_size(mode: str) -> int:\n",
    "    if \"-8\"  in mode: return 8\n",
    "    if \"-16\" in mode: return 16\n",
    "    # For pipeline-parallel point-to-point assume two ranks\n",
    "    return 2\n",
    "\n",
    "df[\"Group\"] = df[\"Mode\"].map(_group_size)\n",
    "\n",
    "df[\"Total_MB\"] = df[\"PerGPU_MB\"] * df[\"Group\"]\n",
    "\n",
    "# Show binary MiB alongside decimal MB\n",
    "MB2MiB = 1_000_000 / 1_048_576         # ≈ 0.953674\n",
    "df[\"PerGPU_MiB\"] = df[\"PerGPU_MB\"] * MB2MiB\n",
    "\n",
    "# Re-order columns for readability\n",
    "cols = [\"Layer\",\"Mode\",\n",
    "        \"PerGPU_MB\",\n",
    "        \"PerGPU_MiB\",\n",
    "        \"Latency_ms\",\"Energy_mJ\"]\n",
    "df = df[cols]\n",
    "\n",
    "display(df.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picotron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
