{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd8fb1c6",
   "metadata": {},
   "source": [
    "### Hop-Typed Communication Model (NVLink vs NDR) for **LLaMA-3 8B**\n",
    "\n",
    "This notebook estimates the communication **latency** and **energy** incurred when training or serving LLaMA-3 8B on a DGX-H100.  Each network hop is classified as **NVLink 4** or **InfiniBand NDR 400**, allowing the model to account for the very different bandwidths, startup latencies, and energy costs of those two links.\n",
    "\n",
    "---\n",
    "\n",
    "#### Parallelism Strategies Examined\n",
    "| Strategy | Communication Pattern (per transformer block) |\n",
    "|----------|----------------------------------------------|\n",
    "| **Data Parallel (DP)** | All-reduce of full-precision gradients across all GPUs. |\n",
    "| **Tensor Parallel (TP)** | All-gather the input activations, then reduce-scatter the output activations (equivalent computation as all-reduce) |\n",
    "| **Pipeline Parallel (PP)** | Point-to-point transfer of activation checkpoints between consecutive pipeline stages. *Pipeline bubbles are **not** modeled.* |\n",
    "---\n",
    "\n",
    "We modeled the forward pass for Tensor and Pipeline, and backward pass for Data Parallelism. The backward pass communication is identical for Tensor and Pipeline parallelism, whilst Data Parallelism has no forward pass communication.\n",
    "\n",
    "#### Model Scope\n",
    "- **Architecture:** LLaMA-3 8B  \n",
    "  – 32 transformer layers, hidden size = 4096, multi-query attention  \n",
    "- **Numerics:** FP16 / BF16 (2 B per element)  \n",
    "- **Sequence Length:** 2048 tokens (for activation sizing)\n",
    "\n",
    "---\n",
    "\n",
    "#### Hardware Link Constants\n",
    "| Link            | Peak BW | Startup \\( \\alpha \\) | Inverse BW \\( \\beta = 1/\\text{BW} \\) | Energy/bit |\n",
    "|-----------------|---------|----------------------|--------------------------------------|------------|\n",
    "| **NVLink 4**    | 900 GB/s | 0.2 µs | 11 ps/B | 5 pJ |\n",
    "| **NDR 400**     | 50 GB/s  | 2 µs  | 20 ps/B | 25 pJ |\n",
    "\n",
    "A message of size \\(M\\) bytes traversing \\(n_{\\text{NV}}\\) NVLink hops and \\(n_{\\text{IB}}\\) InfiniBand hops incurs  \n",
    "\n",
    "$\n",
    "\\text{Latency} = n_{\\text{NV}}\\bigl(\\alpha_{NV} + \\beta_{NV}M \\bigr) \\;+\\; n_{\\text{IB}}\\bigl(\\alpha_{IB} + \\beta_{IB}M \\bigr),\n",
    "$\n",
    "\n",
    "$\n",
    "\\text{Energy}  = 8M\\bigl(5\\,n_{\\text{NV}} + 25\\,n_{\\text{IB}}\\bigr)\\text{ pJ}.\n",
    "$\n",
    "\n",
    "---\n",
    "\n",
    "#### Objectives of the Notebook\n",
    "1. **Quantify Communication Cost** — Report latency (µs) and energy (nJ) per message for each strategy and hop mix.  \n",
    "2. **Compare Parallel Schemes** — Highlight how DP, TP, and PP trade off communication time and energy under the same hardware assumptions.  \n",
    "3. **Guide Design Decisions** — Provide first-order numbers that help decide which parallelism (or combination) is appropriate for a given training or inference workload.\n",
    "\n",
    "---\n",
    "\n",
    "#### Notebook Outputs\n",
    "- **Per-Hop Latency** and **Energy** tables for each collective or point-to-point operation.  \n",
    "- **Data Volume** moved (MB) per GPU and in aggregate.  \n",
    "- Consolidated **comparative charts** for DP, TP, and PP to illustrate trade-offs.\n",
    "\n",
    "> **Caveat:**  Results are first-order; they do not model overlap, asynchronous progress, or pipeline fill/drain bubbles. They nevertheless capture the dominant communication costs needed for quick design-space exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8a9c5b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "# ---------- Model sizes ----------\n",
    "BYTES_FP16 = 2\n",
    "d_model = 4096\n",
    "d_ff    = 14336\n",
    "seq_len = 2048\n",
    "\n",
    "# Self‑attention params (MQA)\n",
    "attn_elems = d_model*d_model + 2*1024*d_model + d_model*d_model\n",
    "attn_bytes = attn_elems * BYTES_FP16\n",
    "\n",
    "# MLP params (no gate)\n",
    "mlp_elems  = d_ff*d_model + d_model*d_ff\n",
    "mlp_bytes  = mlp_elems  * BYTES_FP16\n",
    "\n",
    "# Activation size (one block output) for PP / TP\n",
    "act_bytes = seq_len * d_model * BYTES_FP16\n",
    "\n",
    "layer_bytes = dict(ATTN=attn_bytes, MLP=mlp_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "de7419cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hop tuples (nv, ib)\n",
    "def hops_8():\n",
    "    H = np.empty((8,8),dtype=object)\n",
    "    for i in range(8):\n",
    "        for j in range(8):\n",
    "            if i==j: H[i,j]=(0,0)\n",
    "            else:    H[i,j]=(1,0)   # single NV hop for any pair\n",
    "    return H\n",
    "hop8 = hops_8()\n",
    "\n",
    "def hops_16():\n",
    "    H = np.empty((16,16),dtype=object)\n",
    "    for i in range(16):\n",
    "        for j in range(16):\n",
    "            if i==j: H[i,j]=(0,0)\n",
    "            else:\n",
    "                same_node=(i//8)==(j//8)\n",
    "                if same_node: H[i,j]=(1,0)\n",
    "                else:         H[i,j]=(2,1)  # GPU→NV + IB + NV\n",
    "    return H\n",
    "hop16 = hops_16()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4401d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_nv, beta_nv, epb_nv = 0.2e-6, 1/90e9, 5e-12\n",
    "alpha_ib, beta_ib, epb_ib = 2e-6, 1/50e9, 25e-12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "09dff8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_bytes(lat_bytes, nv_hops, ib_hops):\n",
    "    lat = nv_hops*(alpha_nv+beta_nv*lat_bytes) + ib_hops*(alpha_ib+beta_ib*lat_bytes)\n",
    "    eng = nv_hops*lat_bytes*8*epb_nv + ib_hops*lat_bytes*8*epb_ib\n",
    "    return lat, eng"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "19087651",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows=[]\n",
    "def ring_dp(layer_name, layer_b, hopmat, mode_label):\n",
    "    n = hopmat.shape[0]\n",
    "    chunk = layer_b / n                      # bytes sent per link per step\n",
    "    per_gpu_bytes = layer_b * (n - 1) / n    # textbook formula\n",
    "    steps = 2 * (n - 1)                      # reduce-scatter + all-gather\n",
    "\n",
    "    total_lat = 0.0\n",
    "    total_eng = 0.0\n",
    "\n",
    "    for _ in range(steps):\n",
    "        max_link_lat = 0.0\n",
    "        step_energy  = 0.0\n",
    "\n",
    "        # each rank communicates with its neighbour every step\n",
    "        for r in range(n):\n",
    "            sender    = r\n",
    "            receiver  = (r + 1) % n          # fixed clockwise ring\n",
    "            nv, ib     = hopmat[sender, receiver]\n",
    "            lat, eng   = cost_bytes(chunk, nv, ib)\n",
    "\n",
    "            max_link_lat = max(max_link_lat, lat)\n",
    "            step_energy += eng               # all links consume energy\n",
    "\n",
    "        total_lat += max_link_lat            # critical-path latency\n",
    "        total_eng += step_energy\n",
    "\n",
    "    rows.append([layer_name, mode_label,\n",
    "                 per_gpu_bytes / 1e6,        # MiB\n",
    "                 total_lat  * 1e3,           # µs\n",
    "                 total_eng  * 1e3])          # nJ\n",
    "\n",
    "\n",
    "for L,B in layer_bytes.items():\n",
    "    ring_dp(L,B,hop8,\"DP‑8\")\n",
    "    ring_dp(L,B,hop16,\"DP‑16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2deb6e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_cost(layer_name, bytes_per_msg, hopmat, ring_dp=ring_dp, num_gpus=8):\n",
    "    # reduce‑scatter + all‑reduce = 2 msgs; each msg one NV hop\n",
    "    ring_dp(layer_name, bytes_per_msg, hopmat, \"TP‑\" + str(num_gpus))\n",
    "\n",
    "tp_bytes = act_bytes  # 4096*seq*2\n",
    "tp_cost(\"ATTN\", tp_bytes, hop8, num_gpus=8)\n",
    "tp_cost(\"MLP\" , tp_bytes, hop8, num_gpus=8)\n",
    "tp_cost(\"ATTN\", tp_bytes, hop16, num_gpus=16)\n",
    "tp_cost(\"MLP\" , tp_bytes, hop16, num_gpus=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "009181ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pp_cost(label, nv_hops, ib_hops):\n",
    "    lat,eng = cost_bytes(act_bytes,nv_hops,ib_hops)\n",
    "    rows.append([\"BLOCK\",label,act_bytes/1e6,lat*1e3,eng*1e3])\n",
    "\n",
    "pp_cost(\"PP‑NV\",1,0)\n",
    "pp_cost(\"PP‑Cross\",2,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "666ddda7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer</th>\n",
       "      <th>Mode</th>\n",
       "      <th>PerGPU_MB</th>\n",
       "      <th>PerGPU_MiB</th>\n",
       "      <th>Latency_ms</th>\n",
       "      <th>Energy_mJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>73.400</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.634</td>\n",
       "      <td>46.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>78.643</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6.713</td>\n",
       "      <td>176.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>205.521</td>\n",
       "      <td>196.0</td>\n",
       "      <td>4.570</td>\n",
       "      <td>131.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>220.201</td>\n",
       "      <td>210.0</td>\n",
       "      <td>18.667</td>\n",
       "      <td>493.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>14.680</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.329</td>\n",
       "      <td>9.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>14.680</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.329</td>\n",
       "      <td>9.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>TP‑16</td>\n",
       "      <td>15.729</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>35.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TP‑16</td>\n",
       "      <td>15.729</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>35.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑NV</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑Cross</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.711</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Layer      Mode  PerGPU_MB  PerGPU_MiB  Latency_ms  Energy_mJ\n",
       "0   ATTN      DP‑8     73.400        70.0       1.634     46.976\n",
       "1   ATTN     DP‑16     78.643        75.0       6.713    176.161\n",
       "2    MLP      DP‑8    205.521       196.0       4.570    131.533\n",
       "3    MLP     DP‑16    220.201       210.0      18.667    493.250\n",
       "4   ATTN      TP‑8     14.680        14.0       0.329      9.395\n",
       "5    MLP      TP‑8     14.680        14.0       0.329      9.395\n",
       "6   ATTN     TP‑16     15.729        15.0       1.400     35.232\n",
       "7    MLP     TP‑16     15.729        15.0       1.400     35.232\n",
       "8  BLOCK     PP‑NV     16.777        16.0       0.187      0.671\n",
       "9  BLOCK  PP‑Cross     16.777        16.0       0.711      4.698"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df=pd.DataFrame(rows,columns=[\"Layer\",\"Mode\",\"PerGPU_MB\",\"Latency_ms\",\"Energy_mJ\"])\n",
    "df.round(3)\n",
    "\n",
    "# Derive group size from the Mode label\n",
    "def _group_size(mode: str) -> int:\n",
    "    if \"-8\"  in mode: return 8\n",
    "    if \"-16\" in mode: return 16\n",
    "    # For pipeline-parallel point-to-point assume two ranks\n",
    "    return 2\n",
    "\n",
    "df[\"Group\"] = df[\"Mode\"].map(_group_size)\n",
    "\n",
    "df[\"Total_MB\"] = df[\"PerGPU_MB\"] * df[\"Group\"]\n",
    "\n",
    "# Show binary MiB alongside decimal MB\n",
    "MB2MiB = 1_000_000 / 1_048_576         # ≈ 0.953674\n",
    "df[\"PerGPU_MiB\"] = df[\"PerGPU_MB\"] * MB2MiB\n",
    "\n",
    "# Re-order columns for readability\n",
    "cols = [\"Layer\",\"Mode\",\n",
    "        \"PerGPU_MB\",\n",
    "        \"PerGPU_MiB\",\n",
    "        \"Latency_ms\",\"Energy_mJ\"]\n",
    "df = df[cols]\n",
    "\n",
    "display(df.round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761b0be7",
   "metadata": {},
   "source": [
    "# Adding forward/backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce9b0c11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Layer</th>\n",
       "      <th>Mode</th>\n",
       "      <th>Workload</th>\n",
       "      <th>PerGPU_MB</th>\n",
       "      <th>PerGPU_MiB</th>\n",
       "      <th>Latency_ms</th>\n",
       "      <th>Energy_mJ</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>78.643</td>\n",
       "      <td>75.0</td>\n",
       "      <td>6.713</td>\n",
       "      <td>176.161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>73.400</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1.634</td>\n",
       "      <td>46.976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>220.201</td>\n",
       "      <td>210.0</td>\n",
       "      <td>18.667</td>\n",
       "      <td>493.250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑16</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>205.521</td>\n",
       "      <td>196.0</td>\n",
       "      <td>4.570</td>\n",
       "      <td>131.533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>MLP</td>\n",
       "      <td>DP‑8</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>TP‑16</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>15.729</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>35.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>TP‑16</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>15.729</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>35.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>14.680</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.329</td>\n",
       "      <td>9.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>ATTN</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>14.680</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.329</td>\n",
       "      <td>9.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TP‑16</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>15.729</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>35.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TP‑16</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>15.729</td>\n",
       "      <td>15.0</td>\n",
       "      <td>1.400</td>\n",
       "      <td>35.232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>14.680</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.329</td>\n",
       "      <td>9.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>MLP</td>\n",
       "      <td>TP‑8</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>14.680</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.329</td>\n",
       "      <td>9.395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑Cross</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.711</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑Cross</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.711</td>\n",
       "      <td>4.698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑NV</td>\n",
       "      <td>backward pass</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>BLOCK</td>\n",
       "      <td>PP‑NV</td>\n",
       "      <td>forward pass</td>\n",
       "      <td>16.777</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.187</td>\n",
       "      <td>0.671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Layer      Mode       Workload  PerGPU_MB  PerGPU_MiB  Latency_ms  \\\n",
       "0    ATTN     DP‑16  backward pass     78.643        75.0       6.713   \n",
       "1    ATTN     DP‑16   forward pass      0.000         0.0       0.000   \n",
       "2    ATTN      DP‑8  backward pass     73.400        70.0       1.634   \n",
       "3    ATTN      DP‑8   forward pass      0.000         0.0       0.000   \n",
       "4     MLP     DP‑16  backward pass    220.201       210.0      18.667   \n",
       "5     MLP     DP‑16   forward pass      0.000         0.0       0.000   \n",
       "6     MLP      DP‑8  backward pass    205.521       196.0       4.570   \n",
       "7     MLP      DP‑8   forward pass      0.000         0.0       0.000   \n",
       "8    ATTN     TP‑16  backward pass     15.729        15.0       1.400   \n",
       "9    ATTN     TP‑16   forward pass     15.729        15.0       1.400   \n",
       "10   ATTN      TP‑8  backward pass     14.680        14.0       0.329   \n",
       "11   ATTN      TP‑8   forward pass     14.680        14.0       0.329   \n",
       "12    MLP     TP‑16  backward pass     15.729        15.0       1.400   \n",
       "13    MLP     TP‑16   forward pass     15.729        15.0       1.400   \n",
       "14    MLP      TP‑8  backward pass     14.680        14.0       0.329   \n",
       "15    MLP      TP‑8   forward pass     14.680        14.0       0.329   \n",
       "16  BLOCK  PP‑Cross  backward pass     16.777        16.0       0.711   \n",
       "17  BLOCK  PP‑Cross   forward pass     16.777        16.0       0.711   \n",
       "18  BLOCK     PP‑NV  backward pass     16.777        16.0       0.187   \n",
       "19  BLOCK     PP‑NV   forward pass     16.777        16.0       0.187   \n",
       "\n",
       "    Energy_mJ  \n",
       "0     176.161  \n",
       "1       0.000  \n",
       "2      46.976  \n",
       "3       0.000  \n",
       "4     493.250  \n",
       "5       0.000  \n",
       "6     131.533  \n",
       "7       0.000  \n",
       "8      35.232  \n",
       "9      35.232  \n",
       "10      9.395  \n",
       "11      9.395  \n",
       "12     35.232  \n",
       "13     35.232  \n",
       "14      9.395  \n",
       "15      9.395  \n",
       "16      4.698  \n",
       "17      4.698  \n",
       "18      0.671  \n",
       "19      0.671  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_forward = df.copy()\n",
    "df_backward = df.copy()\n",
    "df_forward['Workload'] = 'forward pass'\n",
    "df_backward['Workload'] = 'backward pass'\n",
    "value_cols = [\"PerGPU_MB\", \"PerGPU_MiB\", \"Latency_ms\", \"Energy_mJ\"]\n",
    "\n",
    "# DP forward pass has 0 communication\n",
    "attn_dp_forward_mask = (df_forward['Workload'].str.contains('forward pass')) & \\\n",
    "                       (df_forward['Mode'].str.contains('DP'))\n",
    "df_forward.loc[attn_dp_forward_mask, value_cols] = 0\n",
    "\n",
    "# Otherwise for TP and PP, the backward pass has the same communication as the forward pass\n",
    "df_processed = pd.concat([df_forward, df_backward], ignore_index=True)\n",
    "\n",
    "cols = [\"Layer\", \"Mode\", \"Workload\",\n",
    "        \"PerGPU_MB\", \"PerGPU_MiB\",\n",
    "        \"Latency_ms\", \"Energy_mJ\"]\n",
    "df_processed = df_processed[cols]\n",
    "\n",
    "# Sort for better comparison with DP first, then TP, then PP\n",
    "def mode_sort_key(mode):\n",
    "    if mode.startswith('DP'):\n",
    "        return 0\n",
    "    elif mode.startswith('TP'):\n",
    "        return 1\n",
    "    elif mode.startswith('PP'):\n",
    "        return 2\n",
    "    return 3\n",
    "\n",
    "df_processed['mode_sort'] = df_processed['Mode'].apply(lambda x: mode_sort_key(x))\n",
    "df_processed = df_processed.sort_values(by=[\"mode_sort\", \"Layer\", \"Mode\", \"Workload\"]).reset_index(drop=True)\n",
    "df_processed = df_processed.drop(columns=['mode_sort'])\n",
    "\n",
    "# 8. Display the final rounded DataFrame\n",
    "display(df_processed.round(3))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picotron",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
